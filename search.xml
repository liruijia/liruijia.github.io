<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Linux下安装Python和Anaconda遇到的坑</title>
    <url>/2021/01/18/Linux%E4%B8%8B%E5%AE%89%E8%A3%85python%E5%92%8Canaconda%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<p>今天本来不打算写文，但是今天发生了一件娱乐圈今天大事:smile:,虽然和我没有什么关系，但是还说想记录一下自己的感受，zs的事情真的跌破三观。</p>
<p>下面还是开始本文吧，这文章主要记录一下，在Linux环境下安装Python和Anaconda遇到的坑！对于Python和Anaconda到底如何安装下面会给出一些连接来供大家参考。</p>
<a id="more"></a>

<h1 id="安装教程"><a href="#安装教程" class="headerlink" title="安装教程"></a>安装教程</h1><p><strong>安装Python3:</strong> </p>
<p>​        <a href="https://www.cnblogs.com/Jimc/p/10218387.html">Linux下安装python3</a>  操作系统Ubuntu</p>
<p>​        <a href="https://www.cnblogs.com/kimyeee/p/7250560.html">Linux下安装Python3.6和第三方库</a> 安装环境Linux</p>
<p><strong>安装Anaconda:</strong></p>
<p>​        <a href="https://blog.csdn.net/Ni_hao2017/article/details/103346694">Linux安装anaconda与jupyter notebook配置</a> 提到了Anaconda 和 jupyter notebook的配置</p>
<h1 id="踩到的坑"><a href="#踩到的坑" class="headerlink" title="踩到的坑"></a>踩到的坑</h1><h2 id="问题1-ctype问题"><a href="#问题1-ctype问题" class="headerlink" title="问题1 ctype问题"></a>问题1 ctype问题</h2><p>python 编译过程（执行make &amp;&amp; make install）中， ctype报错问题</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在python安装目录下执行：</span></span><br><span class="line">yum -y install zlib* </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行下面命令，此处换成你自己的python3安装路径 ，并<span class="built_in">cd</span>到Modules/Setup目录下</span> </span><br><span class="line">vim /usr/local/python3/Python-3.6.8/Modules/Setup  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 进去该文件之后找到下面这一句 去掉注释 退出保存 (esc -&gt; :wq)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz</span>  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">cd</span>到python3的安装路径下 重新编译</span></span><br><span class="line">make &amp; make install </span><br></pre></td></tr></table></figure>
<h2 id="问题2-版本更改"><a href="#问题2-版本更改" class="headerlink" title="问题2 版本更改"></a>问题2 版本更改</h2><p>安装完毕之后，设置python3为默认的版本</p>
<p>Linux系统下一般都会自带python2的版本，但是一般情况下很少用到这个版本，但是不能删除，因为有的应用程序执行代码还是需要到这个版本的python（比如，执行datax任务的时候，datax.py文件是用python2编译的）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除python旧链 系统配置文件路径（不变）</span></span><br><span class="line">rm -rf /usr/bin/python </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建python3 软链</span></span><br><span class="line">ln -s /usr/local/python3/bin/python3  /usr/bin/python  # 前面的路径是你python3的安装路径，cd到bin/python3路径下</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">软链检查路径</span></span><br><span class="line">whereis python </span><br></pre></td></tr></table></figure>
<p>完成上述过程之后，在任意路径下执行<code>python</code>，即可进入python空间，通过可以看到python的版本，出现如下所图说明安装成功。下图中框出来两个部分，分别表示python3和anaconda3安装成功！</p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122173802791-1338678861.png">  </p>
<h2 id="问题3-激活虚拟环境"><a href="#问题3-激活虚拟环境" class="headerlink" title="问题3 激活虚拟环境"></a>问题3 激活虚拟环境</h2><p>anaconda安装虚拟环境之后，激活不了虚拟环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">执行如下命令  激活activate环境 执行完此命令之后，会出现上图中 <span class="string">&quot;(base)&quot;</span> 表示anaconda的基本环境</span></span><br><span class="line">source activate</span><br><span class="line"></span><br><span class="line">conda deactivate # 退出base环境</span><br><span class="line"></span><br><span class="line">conda activate 你的环境名  # 激活你自己建的虚拟环境</span><br></pre></td></tr></table></figure>
<h2 id="问题4-配置阿里源"><a href="#问题4-配置阿里源" class="headerlink" title="问题4 配置阿里源"></a>问题4 配置阿里源</h2><p>配置阿里源的主要原因是阿里源是真的快！！！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pip配置</span></span><br><span class="line">pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">conda配置</span></span><br><span class="line">conda config --add channels http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">conda config --set show_channel_urls yes </span><br></pre></td></tr></table></figure>
<h2 id="问题5-jupyter-notebook-root"><a href="#问题5-jupyter-notebook-root" class="headerlink" title="问题5 jupyter notebook - root"></a>问题5 jupyter notebook - root</h2><p>打开jupyter notebook，出现root问题</p>
<p><code>Running as **root** is not recommended. Use –allow-root to bypass</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">执行以下命令</span></span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">在生成的 jupyter_notebook_config.py 修改配置</span></span><br><span class="line">vim /root/.jupyter/jupyter_notebook_config.py</span><br><span class="line">将c.NotebookApp.allow_root = False前边的  &#x27;#&#x27;去掉，在把False修改为True</span><br></pre></td></tr></table></figure>
<h2 id="问题6-配置jupyter"><a href="#问题6-配置jupyter" class="headerlink" title="问题6 配置jupyter"></a>问题6 配置jupyter</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 任意路径下输入 ，此命令会在 /root/.jupyter 路径下生成一个py文件</span></span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入python编译环境</span> </span><br><span class="line">python</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入编辑环境之后执行：</span></span><br><span class="line">from notebook.auth import passwd  </span><br><span class="line">passwd()  # 输入你自己的命令，并保存最后的hash结果 ，如下面展示的一样，是哈希之后的密码&quot;argon2:$argon2id$v=19$m=10240,t=10,p=8$g9crgnSC05ADyr3kR3qSZQ$PaU8QhZOEbZrAqQrgHHyeQ&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出编译环境</span></span><br><span class="line">exit()</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 编译 /root/.jupyter/jupyter_notebook_config.py</span> </span><br><span class="line">vim /root/.jupyter/jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入之后，找到如下行， 或者 直接复制进行 修改参数即可</span></span><br><span class="line">c.NotebookApp.ip=&#x27;你自己的服务器IP&#x27;       # 也可以设置成&#x27;*&#x27;表示所有ip皆可访问，但是还是建议写成自己的服务器IP,这样好调用</span><br><span class="line">c.NotebookApp.password = &#x27;sha:ce...     # 上面复制hash之后的密码 </span><br><span class="line">c.NotebookApp.open_browser = False       # 禁止自动打开浏览器  </span><br><span class="line">c.NotebookApp.port =8888                 # 端口 不一定要写成8888 ， 你可以自己设定一个端口</span><br><span class="line">c.NotebookApp.notebook_dir = &#x27;/home/xxx&#x27; #在进入到jupyter环境中的公开目录 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置完上面信息之后 保存退出</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>踩坑日记</category>
        <category>python系</category>
        <category>安装日记</category>
        <category>python系</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-DataStream API接口开发规范</title>
    <url>/2021/01/17/Flink_DataStream%20API%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/</url>
    <content><![CDATA[<p>Flink 提供了丰富的客户端操作来提交任务和与任务进行交互,利用flink进行实时统计之前先对其接口开发流程进行了解，Flink提供了Java/sql接口，下面主要就Java-api进行介绍。有不太正确的地方忘纠正！！！</p>
<a id="more"></a>

<h1 id="基本的开发流程"><a href="#基本的开发流程" class="headerlink" title="基本的开发流程"></a>基本的开发流程</h1><p>流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中。<br>但是实际的情况可能会比较复杂一点，一个算子可能有多个实例，因此在真实的计算的时候，不同算子之间的不同实例间会进行数据交换。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。</p>
<p>基于 Apache Storm 用户需要在图中添加 Spout 或 Bolt 这种算子，并指定算子之前的连接方式。这样，在完成整个图的构建之后，就可以将图提交到远程或本地集群运行。与之对比，Apache Flink 的接口虽然也是在构建计算逻辑图，但是 Flink 的 API 定义更加面向数据本身的处理逻辑，它把数据流抽象成为一个无限集，然后定义了一组集合上的操作，然后在底层自动构建相应的 DAG 图。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 构造表达DAG</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 创建 StreamExecutionEnvironment  </span><br><span class="line">设置运行环境  + 可以用来设置参数和创建数据源以及提交任务 </span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 配置数据源读取数据</span><br><span class="line">	DataStream&lt;String&gt; text = env.readTextFile (<span class="string">&quot;input&quot;</span>);</span><br><span class="line">	DataStream&lt;String&gt; text = env.addSource(<span class="keyword">new</span> DataSource()) <span class="comment">// 参数为 自定义的模拟数据源的方法</span></span><br><span class="line">或者</span><br><span class="line">	DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span>处理数据 进行一系列转换 </span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(<span class="keyword">new</span> Tokenizer()).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter()).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line">（这个地方的处理数据的函数 ， 可以自定义 也可以 使用 API）</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> 配置数据汇写出数据</span><br><span class="line">counts.writeAsText(<span class="string">&quot;output&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> 提交执行</span><br><span class="line">env.execute(<span class="string">&quot;Java WordCount from SocketTextStream Example&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WordCount </span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter()).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。<br>Flink调用算子机会在DAG图中建立添加相应的算子节点。<br>前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。<br>Flink 累计计算， Flink 在进行处理的时候，每一次按照设定的时间窗口进行相应的算子，当新的数据进来之后，在原来的基础上进行相应的计算，达到累计的效果。window主要达到了切分的效果，通过过 Window 操作对流可以按时间或者个数进行一些切分，从而将流切分成一个个较小的分组。具体的切分逻辑可以由用户进行选择。当一个分组中所有记录都到达后，用户可以拿到该分组中的所有记录，从而可以进行一些遍历或者累加操作。这样，对每个分组的处理都可以得到一组输出数据，这些输出数据形成了一个新的基本流。</p>
<h1 id="DataStream操作分类"><a href="#DataStream操作分类" class="headerlink" title="DataStream操作分类"></a>DataStream操作分类</h1><p>Flink DataStream API 的核心，就是代表流数据的 DataStream 对象。整个计算逻辑图的构建就是围绕调用 DataStream 对象上的不同操作产生新的 DataStream 对象展开的。DataStream 上的操作可以分为四类 ： </p>
<ul>
<li><p>对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）</p>
</li>
<li><p>对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。通过 Window 将需要的记录关联到一起进行处</p>
</li>
<li><p>对多个流进行操作并转换为单个流，例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作。</p>
</li>
<li><p>DataStream 还支持与合并对称的操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理</p>
<p><img src="img/1608362652106-87285486-8add-43df-a2ac-80c2085e5037.png"></p>



</li>
</ul>
<p>对于普通的 DataStream，我们必须使用 allWindow 操作，它代表对整个流进行统一的 Window 处理，因此是不能使用多个算子实例进行同时计算的。针对这一问题，就需要我们首先使用 KeyBy 方法对记录按 Key 进行分组，然后才可以并行的对不同 Key 对应的记录进行单独的 Window 操作。KeyBy 操作是我们日常编程中最重要的操作之一。</p>
<h1 id="DataStream的分组方式"><a href="#DataStream的分组方式" class="headerlink" title="DataStream的分组方式"></a>DataStream的分组方式</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">keyby —— 为了能够在多个并发实例上并行的对数据进行处理，我们需要通过 KeyBy 将数据进行分组 ，KeyBy 是在水平分向对流进行切分。使用 KeyBy 进行数据切分之后，后续算子的每一个实例可以只处理特定 Key 集合对应的数据</span><br><span class="line"></span><br><span class="line">Global: 上游算子将所有记录发送给下游算子的第一个实例。</span><br><span class="line"></span><br><span class="line">Broadcast: 上游算子将每一条记录发送给下游算子的所有实例。</span><br><span class="line"></span><br><span class="line">Forward：只适用于上游算子实例数与下游算子相同时，每个上游算子实例将记录发送给下游算子对应的实例。</span><br><span class="line"></span><br><span class="line">Shuffle：上游算子对每条记录随机选择一个下游算子进行发送。</span><br><span class="line"></span><br><span class="line">Rebalance：上游算子通过轮询的方式发送数据。</span><br><span class="line"></span><br><span class="line">Rescale：当上游和下游算子的实例数为 n 或 m 时，如果 n &lt; m，则每个上游实例向ceil(m/n)或floor(m/n)个下游实例轮询发送数据；如果 n &gt; m，则 floor(n/m) 或 ceil(n/m) 个上游实例向下游实例轮询发送数据。</span><br><span class="line"></span><br><span class="line">PartitionCustomer：当上述内置分配方式不满足需求时，用户还可以选择自定义分组方式。</span><br></pre></td></tr></table></figure>
<h1 id="DataStream的类型"><a href="#DataStream的类型" class="headerlink" title="DataStream的类型"></a>DataStream的类型</h1><p>Flink DataStream 对像都是强类型的，每一个 DataStream 对象都需要指定元素的类型，Flink 自己底层的序列化机制正是依赖于这些信息对序列化等进行优化。具体来说，在 Flink 底层，它是使用 TypeInformation 对象对类型进行描述的，TypeInformation 对象定义了一组类型相关的信息供序列化框架使用。</p>
<p>![](<a href="https://cdn.nlark.com/yuque/0/2020/png/1792867/1608363333508-df79c40f-18dc-41bc-91e6-dce533daec04.png#align=left&amp;display=inline&amp;height=405&amp;margin=[object">https://cdn.nlark.com/yuque/0/2020/png/1792867/1608363333508-df79c40f-18dc-41bc-91e6-dce533daec04.png#align=left&amp;display=inline&amp;height=405&amp;margin=[object</a> Object]&amp;originHeight=405&amp;originWidth=720&amp;size=0&amp;status=done&amp;style=none&amp;width=720)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupedProcessingTimeWindowSample</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 模拟数据源  extends - 继承 数据源</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="comment">// Flink 在运行时对 Source 会直接调用该方法，该方法需要不断的输出数据，从而形成初始的流 </span></span><br><span class="line">        <span class="comment">// 随机的产生商品类别和交易量的数据</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Random random = <span class="keyword">new</span> Random();</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="comment">//getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态</span></span><br><span class="line">                Thread.sleep((getRuntimeContext().getIndexOfThisSubtask() + <span class="number">1</span>) * <span class="number">1000</span> * <span class="number">5</span>); <span class="comment">// 不知道 是否可以看作是window</span></span><br><span class="line">                String key = <span class="string">&quot;类别&quot;</span> + (<span class="keyword">char</span>) (<span class="string">&#x27;A&#x27;</span> + random.nextInt(<span class="number">3</span>));</span><br><span class="line">                <span class="keyword">int</span> value = random.nextInt(<span class="number">10</span>) + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">                System.out.println(String.format(<span class="string">&quot;Emits\t(%s, %d)&quot;</span>, key, value));</span><br><span class="line">                <span class="comment">// 将结果通过ctx.collect 方法进行发送</span></span><br><span class="line">                ctx.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="comment">// 当 Flink 需要 Cancel Source Task 的时候会调用该方法</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 构建图</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        </span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 添加数据源 </span></span><br><span class="line">        <span class="comment">// 基本数据源</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = env.addSource(<span class="keyword">new</span> DataSource()); </span><br><span class="line">        <span class="comment">// 分组数据源 按照第一个字段进行分组</span></span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(<span class="number">0</span>); </span><br><span class="line">		</span><br><span class="line">        <span class="comment">// keyedStream 按照第二个字段进行求和</span></span><br><span class="line">        keyedStream.sum(<span class="number">1</span>).keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Object <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).fold(<span class="keyword">new</span> HashMap&lt;String, Integer&gt;(), <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="comment">// 利用flod 来维持数据的真实性</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> HashMap&lt;String, Integer&gt; <span class="title">fold</span><span class="params">(HashMap&lt;String, Integer&gt; accumulator, Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// hashmap 对于当前的数据进行维护，当有新的记录进来之后进行更新</span></span><br><span class="line">                accumulator.put(value.f0, value.f1);</span><br><span class="line">                <span class="keyword">return</span> accumulator;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="comment">// 最终结果输出</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(HashMap&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                  <span class="comment">// 每个类型的商品成交量</span></span><br><span class="line">                  System.out.println(value);</span><br><span class="line">                  <span class="comment">// 商品成交总量                  </span></span><br><span class="line">                  System.out.println(value.values().stream().mapToInt(v -&gt; v).sum());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在此之上还有更上一层的API接口，基于table和SQL </p>
]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Datax数据同步</title>
    <url>/2021/01/17/datax%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>datax 数据同步离线工具，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。</p>
<a id="more"></a>

<h1 id="Datax介绍"><a href="#Datax介绍" class="headerlink" title="Datax介绍"></a>Datax介绍</h1><p>datax 数据同步离线工具，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。</p>
<p>​    安装部署-<a href="http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz">安装路径</a>,只需要进行相应的安装解压到相应的路径即可。每次执行任务则是将使用<code>/datax/bin/data.py</code>文件来去执行相应的任务即可。</p>
<p>​    DataX执行同步任务得时候，将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。</p>
<ul>
<li> Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。</li>
<li>Writer：Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。</li>
<li>Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。</li>
</ul>
]]></content>
      <categories>
        <category>大数据学习</category>
      </categories>
      <tags>
        <tag>数据同步</tag>
        <tag>Datax</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库</title>
    <url>/2021/01/17/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<p>数据仓库在同步表的时候，针对于不同的情况分为增量表和全量表，下面将对于全量和增量的概念进行一一阐述！</p>
<a id="more"></a>

<h1 id="全量与增量"><a href="#全量与增量" class="headerlink" title="全量与增量"></a>全量与增量</h1><p>在进行数据同步的时候，通常会提到两个词，全量同步和增量同步，到底全量和增量到底有何区别。在此之前先对以下几个概念有个了解：</p>
<p>（1）存量：系统在某一时点时的所保有的数量；</p>
<p>（2）流量：是指在某一段时间内流入/出系统的数量</p>
<p>（3）增量：则是指在某一段时间内系统中保有数量的变化</p>
<p>（4）增量=流入量–流出量</p>
<p>（5）本期期末存量=上期期末存量+本期内增量</p>
<h1 id="全量"><a href="#全量" class="headerlink" title="全量"></a>全量</h1><p>常说的全量表：即当前时刻存放了所有的历史记录数据。 每一次在同步数据的时候，都要将全部的数据进行拉取，这样做的好处就是，每一次进行查询的时候，表里存放了所有的历史数据，不需要再和之前的数据进行关联，便于SQL的书写，但是这样做的不好之处在于，每次都要拉取全部数据，很耗资源，耗时长，当数据量达到一定程度的时候，这样做根本是不行的。</p>
<p>对于一个全量表来说，可以总结成以下三点：</p>
<p>（1）有无变化，都要报</p>
<p>（2）每一次查询的时候都是所有的历史数据</p>
<p>（3）只有一个分区</p>
<h1 id="增量表"><a href="#增量表" class="headerlink" title="增量表"></a>增量表</h1><p> 新增数据，增量数据是上次导出之后的新数据。每一次同步的时候，只是将当天的变化数据进行同步，加入一个用户在昨天进行了一次操作，今天又进行了一次操作，那按照全量的意思来讲，今天在全量表中进行查询的时候其存在两条该用户的数据，而增量表在当天的分区中只有一条该用户的数据，昨天的那条用户数据需要在昨天的分区中进行查找。</p>
<p>因此，一个增量表有如下的几个特征:</p>
<p> (1）记录每次增加的量，而不是总量；</p>
<p> (2）增量表，只报变化量，无变化不用报</p>
<p> (3）每天一个分区</p>
<h1 id="3-快照表"><a href="#3-快照表" class="headerlink" title="3.快照表"></a>3.快照表</h1><p>   快照表的存在是为了备份的全量表，其和全量表并无任何差别，只是，快照表为了便于存储数据，按日分区，记录截止数据日期的全量数据。总的来说快照表有如下三个特点：</p>
<p>（1）快照表，有无变化，都要报</p>
<p>（2）每次上报的数据都是所有的数据（变化的 + 没有变化的）</p>
<p>（3）一天一个分区</p>
<h1 id="4-拉链表"><a href="#4-拉链表" class="headerlink" title="4.拉链表"></a>4.拉链表</h1><pre><code>     在有些情况下，为了保持历史的一些状态，需要用拉链表来做，这样做目的在可以保留所有状态的情况下可以节省空间。  </code></pre>
<p>​        拉链表，记录一个事件从开始到现在为止的历史变化信息，感觉像是快照表 / 全量表 ，但是拉链表在快照表上进行了优化，在去重的前提下又保留了所有事件从开始到当前的变化信息 ；一般这种表有每一个事件开始活跃到当前的时间信息，记录截止数据日期的全量数据。<br>拉链表适用于以下几种情况吧   数据量有点大，表中某些字段有变化，但是呢变化的频率也不是很高，业务需求呢又需要统计这种变化状态，每天全量一份呢，有点不太现实，不仅浪费了存储空间，有时可能业务统计也有点麻烦，这时，拉链表的作用就提现出来了，既节省空间，又满足了需求。 总的来说，拉链表有如下几个特征：</p>
<p>（1）记录一个事物从开始，一直到当前状态的所有变化的信息；</p>
<p>（2）拉链表每次上报的都是历史记录的最终状态，是记录在当前时刻的历史总量；</p>
<p>（3）当前记录存的是当前时间之前的所有历史记录的最后变化量（总量）；</p>
<p>（4）只有一个分区</p>
]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>全量</tag>
        <tag>增量</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-DataStream API 学习</title>
    <url>/2021/01/17/Flink-DataStream%20API%20%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Flink 提供了丰富的客户端操作来提交任务和与任务进行交互，包括 Flink 命令行，Scala Shell，SQL Client，Restful API 和 Web。Flink 首先提供的最重要的是命令行，其次是 SQL Client 用于提交 SQL 任务的运行，还有就是 Scala Shell 提交 Table API 的任务。同时，Flink 也提供了 Restful 服务，用户可以通过 http 方式进行调用。此外，还有 Web 的方式可以提交任务。</p>
<a id="more"></a>

<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p> 在之前写过，Flink DataStream API 在进行实时计算的时候，需要先建立相应的DAG环境，然后进行DataStream的读取，进而进行相应的数据实时统计 ！！！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure>
<p>参考： <code>https://xieyuanpeng.com/2019/02/05/flink-learning-3/</code></p>
<h1 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h1><h2 id="fromElements"><a href="#fromElements" class="headerlink" title="fromElements"></a>fromElements</h2><p>从批中读取数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; stream = env</span><br><span class="line">  .fromElements(</span><br><span class="line">    <span class="keyword">new</span> SensorReading(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1547718199</span>, <span class="number">35.80018327300259</span>),</span><br><span class="line">    <span class="keyword">new</span> SensorReading(<span class="string">&quot;sensor_6&quot;</span>, <span class="number">1547718199</span>, <span class="number">15.402984393403084</span>),</span><br><span class="line">    <span class="keyword">new</span> SensorReading(<span class="string">&quot;sensor_7&quot;</span>, <span class="number">1547718199</span>, <span class="number">6.720945201171228</span>),</span><br><span class="line">    <span class="keyword">new</span> SensorReading(<span class="string">&quot;sensor_10&quot;</span>, <span class="number">1547718199</span>, <span class="number">38.101067604893444</span>)</span><br><span class="line">  )</span><br><span class="line"> <span class="comment">// SensorReading 数据流的类型</span></span><br><span class="line"> SensorReading(<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="readTextFile"><a href="#readTextFile" class="headerlink" title="readTextFile"></a>readTextFile</h2><p>从文件中读取数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 直接通过相应的路径</span></span><br><span class="line">DataStream&lt;String&gt; stream = env.readTextFile(filePath);</span><br><span class="line"><span class="comment">// 通过参数params传入</span></span><br><span class="line">carData = env.readTextFile(params.get(<span class="string">&quot;input&quot;</span>)).map(<span class="keyword">new</span> TopSpeedWindowing.ParseCarData())</span><br></pre></td></tr></table></figure>
<h2 id="addsource"><a href="#addsource" class="headerlink" title="addsource"></a>addsource</h2><p>addcource 添加数据源，可以添加自定义的数据源</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;String&gt; text = env.addSource(<span class="keyword">new</span> DataSource())</span><br><span class="line"><span class="comment">// DataSource为一个自定义的获取数据的函数，在进行自定义的时候，需要使用RichParallelSourceFunction这个方法</span></span><br></pre></td></tr></table></figure>
<p><strong>SensorSource类：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line"><span class="comment">// 自定义数据源，需要完成run &amp; cancel 方法</span></span><br><span class="line"><span class="comment">// 示例1 </span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Calendar;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">SensorReading</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// SensorReading 最后输出的数据流 类型， srcCtx 用来提交最后的数据流</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;SensorReading&gt; srcCtx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Random rand = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">        String[] sensorIds = <span class="keyword">new</span> String[<span class="number">10</span>];</span><br><span class="line">        <span class="keyword">double</span>[] curFTemp = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">10</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            sensorIds[i] = <span class="string">&quot;sensor_&quot;</span> + i;</span><br><span class="line">            curFTemp[i] = <span class="number">65</span> + (rand.nextGaussian() * <span class="number">20</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">long</span> curTime = Calendar.getInstance().getTimeInMillis();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                curFTemp[i] += rand.nextGaussian() * <span class="number">0.5</span>;</span><br><span class="line">                srcCtx.collect(<span class="keyword">new</span> SensorReading(sensorIds[i], curTime, curFTemp[i]));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例2</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupedProcessingTimeWindowSample</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 模拟数据源  extends - 继承 数据源</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="comment">// Flink 在运行时对 Source 会直接调用该方法，该方法需要不断的输出数据，从而形成初始的流</span></span><br><span class="line">        <span class="comment">// 随机的产生商品类别和交易量的数据</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Random random = <span class="keyword">new</span> Random();</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="comment">//getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态</span></span><br><span class="line">                Thread.sleep((getRuntimeContext().getIndexOfThisSubtask() + <span class="number">1</span>) * <span class="number">1000</span> * <span class="number">5</span>); <span class="comment">// 不知道 是否可以看作是window</span></span><br><span class="line">                String key = <span class="string">&quot;类别&quot;</span> + (<span class="keyword">char</span>) (<span class="string">&#x27;A&#x27;</span> + random.nextInt(<span class="number">3</span>));</span><br><span class="line">                <span class="keyword">int</span> value = random.nextInt(<span class="number">10</span>) + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">                System.out.println(String.format(<span class="string">&quot;Emits\t(%s, %d)&quot;</span>, key, value));</span><br><span class="line">                <span class="comment">// 将结果通过ctx.collect 方法进行发送</span></span><br><span class="line">                ctx.collect(<span class="keyword">new</span> Tuple2&lt;String , Integer&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="comment">// 当 Flink 需要 Cancel Source Task 的时候会调用该方法</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h1><p>DataStream API针对大多数数据转换操作提供了转换算子。如果你很熟悉批处理API、函数式编程语言或者SQL，那么你将会发现这些API很容易学习。我们会将DataStream API的转换算子分成四类：</p>
<ul>
<li><strong>基本转换算子</strong>：将会作用在数据流中的每一条单独的数据上。</li>
<li><strong>KeyedStream转换算子</strong>：在数据有key的情况下，对数据应用转换算子。</li>
<li><strong>多流转换算子</strong>：合并多条流为一条流或者将一条流分割为多条流。</li>
<li><strong>分布式转换算子</strong>：将重新组织流里面的事件。</li>
</ul>
<h2 id="基本转换算子"><a href="#基本转换算子" class="headerlink" title="基本转换算子"></a>基本转换算子</h2><p>基本转换算子会针对流中的每一个单独的事件做处理，也就是说每一个输入数据会产生一个输出数据。单值转换，数据的分割，数据的过滤，都是基本转换操作的典型例子</p>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p><code>map</code>算子通过调用<code>DataStream.map()</code>来指定。<code>map</code>算子的使用将会产生一条新的数据流。它会将每一个输入的事件传送到一个用户自定义的mapper，这个mapper只返回一个输出事件，这个输出事件和输入事件的类型可能不一样。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 行内函数 map 函数 ，它使用 Lambda 表达式进行相应的计算</span></span><br><span class="line"><span class="comment">// 1:1</span></span><br><span class="line">env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    .map(i -&gt; Tuple2.of(i, i))</span><br><span class="line">    .returns(Types.TUPLE(Types.INT, Types.INT))</span><br><span class="line">    .print();</span><br><span class="line"><span class="comment">// 上面这段代码， 数据为1 2 3 ,map 进行映射处理 计算i-&gt;i*i , 最后进行返回，数据格式限制成int ,print打印结果</span></span><br></pre></td></tr></table></figure>
<h3 id="FILTER"><a href="#FILTER" class="headerlink" title="FILTER"></a>FILTER</h3><p><code>filter</code>转换算子通过在每个输入事件上对一个布尔条件进行求值来过滤掉一些元素，然后将剩下的元素继续发送。一个<code>true</code>的求值结果将会把输入事件保留下来并发送到输出，而如果求值结果为<code>false</code>，则输入事件会被抛弃掉。我们通过调用<code>DataStream.filter()</code>来指定流的<code>filter</code>算子，<code>filter</code>操作将产生一条新的流，其类型和输入流中的事件类型是一样的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1:1</span></span><br><span class="line">DataStream&lt;SensorReading&gt; filteredReadings = readings.filter(r -&gt; r.temperature &gt;= <span class="number">25</span>);</span><br></pre></td></tr></table></figure>
<h3 id="FLATMAP"><a href="#FLATMAP" class="headerlink" title="FLATMAP"></a>FLATMAP</h3><p><code>flatMap</code>算子和<code>map</code>算子很类似，不同之处在于针对每一个输入事件<code>flatMap</code>可以生成0个、1个或者多个输出元素。事实上，<code>flatMap</code>转换算子是<code>filter</code>和<code>map</code>的泛化。所以<code>flatMap</code>可以实现<code>map</code>和<code>filter</code>算子的功能。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// T: the type of input elements</span></span><br><span class="line"><span class="comment">// O: the type of output elements</span></span><br><span class="line"><span class="comment">// 最后的输出是一个Collector集合</span></span><br><span class="line">FlatMapFunction[T, O]</span><br><span class="line">    &gt; flatMap(T, Collector[O]): Unit</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IdSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String id, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] splits = id.split(<span class="string">&quot;_&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">            out.collect(split);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此时调用的时候</span></span><br><span class="line">DataStream.flatMap(<span class="keyword">new</span> IdSplitter())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匿名函数的写法  </span></span><br><span class="line">DataStream&lt;String&gt; splitIds = sensorIds</span><br><span class="line">        .flatMap((FlatMapFunction&lt;String, String&gt;) <span class="comment">// 函数名，输入的格式</span></span><br><span class="line">                (id, out) -&gt; &#123; <span class="keyword">for</span> (String s: id.split(<span class="string">&quot;_&quot;</span>)) &#123; out.collect(s);&#125;&#125;)</span><br><span class="line">        <span class="comment">// provide result type because Java cannot infer return type of lambda function</span></span><br><span class="line">        <span class="comment">// 提供结果的类型，因为Java无法推断匿名函数的返回值类型</span></span><br><span class="line">        .returns(Types.STRING); <span class="comment">// 返回结果</span></span><br></pre></td></tr></table></figure>
<h2 id="键控流转换算子"><a href="#键控流转换算子" class="headerlink" title="键控流转换算子"></a>键控流转换算子</h2><p>很多流处理程序的一个基本要求就是要能对数据进行分组，分组后的数据共享某一个相同的属性。<code>DataStream API</code>提供了一个叫做<code>KeyedStream</code>的抽象，此抽象会从逻辑上对<code>DataStream</code>进行分区，分区后的数据拥有同样的<code>Key</code>值，分区后的流互不相关。<br>针对<code>KeyedStream</code>的状态转换操作可以读取数据或者写入数据到当前事件<code>Key</code>所对应的状态中。这表明拥有同样<code>Key</code>的所有事件都可以访问同样的状态，也就是说所以这些事件可以一起处理。<br><code>KeyedStream</code>可以使用<code>map</code>，<code>flatMap</code>和<code>filter</code>算子来处理。接下来我们会使用<code>keyBy</code>算子来将<code>DataStream</code>转换成<code>KeyedStream</code>.</p>
<h3 id="keyby-—-分流"><a href="#keyby-—-分流" class="headerlink" title="keyby — 分流"></a>keyby — 分流</h3><p><code>keyBy</code>通过指定<code>key</code>来将<code>DataStream</code>转换成<code>KeyedStream</code>,基于不同的<code>key</code>，流中的事件将被分配到不同的分区中去。所有具有相同<code>key</code>的事件将会在接下来的操作符的同一个子任务槽中进行处理。拥有不同<code>key</code>的事件可以在同一个任务中处理。但是算子只能访问当前事件的key所对应的状态。<br><code>keyBy()</code>方法接收一个参数，这个参数指定了key或者keys，有很多不同的方法来指定key。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">KeyedStream&lt;SensorReading, String&gt; keyed = readings.keyBy(r -&gt; r.id); <span class="comment">// 指定按照r.id为分组依据</span></span><br><span class="line"></span><br><span class="line">st = stream.flatMap().keyby() </span><br><span class="line"><span class="comment">// 对stream进行flatmap中相应的处理之后，利用keyby分组 这个分组指的是对于进来的数据流进行分流 分到不同的流中进行计算</span></span><br></pre></td></tr></table></figure>
<p>基于<code>key</code>的两种操作：滚动聚合和<code>reduce</code>算子，滚动聚合算子由<code>KeyedStream</code>调用，并生成一个聚合以后的<code>DataStream</code>，例如：<code>sum</code>，<code>minimum</code>，<code>maximum</code>。一个滚动聚合算子会为每一个观察到的<code>key</code>保存一个聚合的值。</p>
<h3 id="滚动聚合算子-—-聚合"><a href="#滚动聚合算子-—-聚合" class="headerlink" title="滚动聚合算子 — 聚合"></a>滚动聚合算子 — 聚合</h3><p>常用的滚动聚合算子有：</p>
<ul>
<li>sum()：在输入流上对指定的字段做滚动相加操作。</li>
<li>min()： 在输入流上对指定的字段求最小值。</li>
<li>max()： 在输入流上对指定的字段求最大值。</li>
<li>minBy()：在输入流上针对指定字段求最小值，并返回包含当前观察到的最小值的事件。</li>
<li>maxBy()：在输入流上针对指定字段求最大值，并返回包含当前观察到的最大值的事件。</li>
</ul>
<p>滚动聚合算子无法组合起来使用，每次计算只能使用一个单独的滚动聚合算子。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; inputStream = env.fromElements(<span class="keyword">new</span> Tuple3(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), <span class="keyword">new</span> Tuple3(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), <span class="keyword">new</span> Tuple3(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), <span class="keyword">new</span> Tuple3(<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>));</span><br><span class="line"><span class="comment">// 数据流：</span></span><br><span class="line"><span class="comment">// 1, 2, 2 </span></span><br><span class="line"><span class="comment">// 2, 3, 1</span></span><br><span class="line"><span class="comment">// 2, 2, 4</span></span><br><span class="line"><span class="comment">// 1，5，3</span></span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; resultStream = inputStream</span><br><span class="line">  .keyBy(<span class="number">0</span>) <span class="comment">// key on first field of the tuple  经过这一步之后，1 2 2 / 1 5 3进入了同一个算子代码槽中进行计算</span></span><br><span class="line">  .sum(<span class="number">1</span>);   <span class="comment">// sum the second field of the tuple in place</span></span><br><span class="line">最后的结果为： </span><br><span class="line">key , sum ,<span class="keyword">else</span> </span><br><span class="line">key = <span class="number">1</span> 时的结果： <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> ,<span class="number">1</span> <span class="number">7</span> <span class="number">2</span> 随着数据流不断地进入 不断地进行累加</span><br><span class="line">key = <span class="number">2</span> 时的结果： <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> , <span class="number">2</span> <span class="number">5</span> <span class="number">1</span> 第三个字段未定义</span><br></pre></td></tr></table></figure>
<p>滚动聚合操作会对每一个key都保存一个状态。因为状态从来不会被清空，所以我们在使用滚动聚合算子时只能使用在含有有限个key的流上面。</p>
<h3 id="REDUCE-—-泛化滚动聚合"><a href="#REDUCE-—-泛化滚动聚合" class="headerlink" title="REDUCE —  泛化滚动聚合"></a>REDUCE —  泛化滚动聚合</h3><p><code>reduce</code>算子是滚动聚合的泛化实现。它将一个<code>ReduceFunction</code>应用到了一个<code>KeyedStream</code>上面去。<code>reduce</code>算子将会把每一个输入事件和当前已经<code>reduce</code>出来的值做聚合计算。<code>reduce</code>操作不会改变流的事件类型。输出流数据类型和输入流数据类型是一样的。<br><code>reduce</code>函数可以通过实现接口<code>ReduceFunction</code>来创建一个类。<code>ReduceFunction</code>接口定义了<code>reduce()</code>方法，此方法接收两个输入事件，输入一个相同类型的事件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// T: the element type</span></span><br><span class="line">ReduceFunction[T]</span><br><span class="line">    &gt; reduce(T, T): T</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; maxTempPerSensor = keyestream</span><br><span class="line">        .reduce((r1, r2) -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (r1.temperature &gt; r2.temperature) &#123;</span><br><span class="line">                <span class="keyword">return</span> r1;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> r2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>
<h2 id="多流转换算子"><a href="#多流转换算子" class="headerlink" title="多流转换算子"></a>多流转换算子</h2><p>许多应用需要摄入多个流并将流合并处理，还可能需要将一条流分割成多条流然后针对每一条流应用不同的业务逻辑</p>
<h3 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h3><p>DataStream.union()方法将两条或者多条DataStream合并成一条具有与输入流相同类型的输出DataStream。事件合流的方式为FIFO方式。操作符并不会产生一个特定顺序的事件流。union操作符也不会进行去重。每一个输入事件都被发送到了下一个操作符。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; parisStream = ...</span><br><span class="line">DataStream&lt;SensorReading&gt; tokyoStream = ...</span><br><span class="line">DataStream&lt;SensorReading&gt; rioStream = ...</span><br><span class="line">DataStream&lt;SensorReading&gt; allCities = parisStream.union(tokyoStream, rioStream)</span><br></pre></td></tr></table></figure>
<h3 id="CONNECT-COMAP和COFLATMAP"><a href="#CONNECT-COMAP和COFLATMAP" class="headerlink" title="CONNECT, COMAP和COFLATMAP"></a>CONNECT, COMAP和COFLATMAP</h3><p>　联合两条流的事件是非常常见的流处理需求。例如监控一片森林然后发出高危的火警警报。报警的Application接收两条流，一条是温度传感器传回来的数据，一条是烟雾传感器传回来的数据。当两条流都超过各自的阈值时，报警。<br><code>DataStream API</code>提供了<code>connect</code>操作来支持以上的应用场景。<code>DataStream.connect()</code>方法接收一条<code>DataStream</code>，然后返回一个<code>ConnectedStreams</code>类型的对象，这个对象表示了两条连接的流。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// first stream</span></span><br><span class="line">DataStream&lt;Integer&gt; first = ...</span><br><span class="line"><span class="comment">// second stream</span></span><br><span class="line">DataStream&lt;String&gt; second = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// connect streams 后面&lt;&gt;里添加的是 两个连接的 数据流的类型</span></span><br><span class="line">ConnectedStreams&lt;Integer, String&gt; connected = first.connect(second);</span><br></pre></td></tr></table></figure>
<p><code>ConnectedStreams</code>提供了<code>map()</code>和<code>flatMap()</code>方法，分别需要接收类型为<code>CoMapFunction</code>和<code>CoFlatMapFunction</code>的参数。以上两个函数里面的泛型是第一条流的事件类型和第二条流的事件类型，以及输出流的事件类型。还定义了两个方法，每一个方法针对一条流来调用。<code>map1()</code>和<code>flatMap1()</code>会调用在第一条流的元素上面，<code>map2()</code>和<code>flatMap2()</code>会调用在第二条流的元素上面。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// IN1: 第一条流的事件类型</span></span><br><span class="line"><span class="comment">// IN2: 第二条流的事件类型</span></span><br><span class="line"><span class="comment">// OUT: 输出流的事件类型</span></span><br><span class="line">CoMapFunction[IN1, IN2, OUT]</span><br><span class="line">    &gt; map1(IN1): OUT</span><br><span class="line">    &gt; map2(IN2): OUT</span><br><span class="line"></span><br><span class="line">CoFlatMapFunction[IN1, IN2, OUT]</span><br><span class="line">    &gt; flatMap1(IN1, Collector[OUT]): Unit</span><br><span class="line">    &gt; flatMap2(IN2, Collector[OUT]): Unit</span><br></pre></td></tr></table></figure>
<p>对两条流做连接查询通常需要这两条流基于某些条件被确定性的路由到操作符中相同的并行实例里面去。在默认情况下，<code>connect()</code>操作将不会对两条流的事件建立任何关系，所以两条流的事件将会随机的被发送到下游的算子实例里面去。这样的行为会产生不确定性的计算结果，显然不是我们想要的。为了针对<code>ConnectedStreams</code>进行确定性的转换操作，<code>connect()</code>方法可以和<code>keyBy()</code>或者<code>broadcast()</code>组合起来使用。</p>
<ul>
<li><p><strong>与keyby()</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Integer, Long&gt;&gt; one = ...</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; two = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// keyBy two connected streams keyby算子操作ConnectStream </span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Int, Long&gt;, Tuple2&lt;Integer, String&gt;&gt; keyedConnect1 = one</span><br><span class="line">  .connect(two)</span><br><span class="line">  .keyBy(<span class="number">0</span>, <span class="number">0</span>); <span class="comment">// key both input streams on first attribute</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// alternative: connect two keyed streams connect 连接 两条keystream</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Integer, Long&gt;, Tuple2&lt;Integer, String&gt;&gt; keyedConnect2 = one</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  .connect(two.keyBy(<span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<p>无论使用keyBy()算子操作ConnectedStreams还是使用connect()算子连接两条KeyedStreams，<strong>connect()算子会将两条流的含有相同Key的所有事件都发送到相同的算子实例。两条流的key必须是一样的类型和值</strong>，就像SQL中的JOIN。在connected和keyed stream上面执行的算子有访问keyed state的权限。</p>
</li>
<li><p><strong>broadcast</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Integer, Long&gt;&gt; one = ...</span><br><span class="line">DataStream&lt;Tuple2&lt;Int, String&gt;&gt; two = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// connect streams with broadcast</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Int, Long&gt;, Tuple2&lt;Int, String&gt;&gt; keyedConnect = first</span><br><span class="line">  <span class="comment">// broadcast second input stream</span></span><br><span class="line">  .connect(second.broadcast());</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>一条被广播过的流中的所有元素将会被复制然后发送到下游算子的所有并行实例中去</strong>。<strong>未被广播过的流仅仅向前发送。</strong>所以两条流的元素显然会被连接处理。</p>
<h2 id="分布式转换算子"><a href="#分布式转换算子" class="headerlink" title="分布式转换算子"></a>分布式转换算子</h2><p>分区操作对应于我们之前讲过的“数据交换策略”这一节。这些操作定义了事件如何分配到不同的任务中去。当我们使用DataStream API来编写程序时，系统将自动的选择数据分区策略，然后根据操作符的语义和设置的并行度将数据路由到正确的地方去。有些时候，我们需要在应用程序的层面控制分区策略，或者自定义分区策略。例如，如果我们知道会发生数据倾斜，那么我们想要针对数据流做负载均衡，将数据流平均发送到接下来的操作符中去。又或者，应用程序的业务逻辑可能需要一个算子所有的并行任务都需要接收同样的数据。再或者，我们需要自定义分区策略的时候。在这一小节，我们将展示DataStream的一些方法，可以使我们来控制或者自定义数据分区策略。<br><strong>keyBy()方法不同于分布式转换算子。所有的分布式转换算子将产生DataStream数据类型。而keyBy()产生的类型是KeyedStream，它拥有自己的keyed state。</strong></p>
<h3 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h3><p>随机数据交换由<code>DataStream.shuffle()</code>方法实现。shuffle方法将数据随机的分配到下游算子的并行任务中去。</p>
<h3 id="Round-Robin"><a href="#Round-Robin" class="headerlink" title="Round-Robin"></a>Round-Robin</h3><p><code>rebalance()</code>方法使用<code>Round-Robin</code>负载均衡算法将输入流平均分配到随后的并行运行的任务中去。下图为<code>round-robin</code>分布式转换算子的示意图。</p>
<h3 id="Rescale"><a href="#Rescale" class="headerlink" title="Rescale"></a>Rescale</h3><p><code>rescale()</code>方法使用的也是<code>round-robin</code>算法，但只会将数据发送到接下来的并行运行的任务中的一部分任务中。本质上，当发送者任务数量和接收者任务数量不一样时，rescale分区策略提供了一种轻量级的负载均衡策略。如果接收者任务的数量是发送者任务的数量的倍数时，rescale操作将会效率更高。<br><code>rebalance()</code>和<code>rescale()</code>的根本区别在于任务之间连接的机制不同。 <code>rebalance()</code>将会针对所有发送者任务和所有接收者任务之间建立通信通道，而<code>rescale()</code>仅仅针对每一个任务和下游算子的一部分子并行任务之间建立通信通道。rescale的示意图如下。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/1792867/1609076902383-c5355879-b6e9-49bd-becf-6ffbdefde447.png#align=left&display=inline&height=731&margin=%5Bobject%20Object%5D&name=image.png&originHeight=731&originWidth=980&size=205246&status=done&style=none&width=980" alt="image.png"></p>
<h3 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h3><p><code>broadcast()</code>方法将输入流的所有数据复制并发送到下游算子的所有并行任务中去。</p>
<h3 id="Global"><a href="#Global" class="headerlink" title="Global"></a>Global</h3><p><code>global()</code>方法将所有的输入流数据都发送到下游算子的第一个并行任务中去。这个操作需要很谨慎，因为将所有数据发送到同一个task，将会对应用程序造成很大的压力。</p>
<h3 id="Custom"><a href="#Custom" class="headerlink" title="Custom"></a>Custom</h3><p>当Flink提供的分区策略都不适用时，我们可以使用<code>partitionCustom()</code>方法来自定义分区策略。这个方法接收一个<code>Partitioner</code>对象，这个对象需要实现分区逻辑以及定义针对流的哪一个字段或者key来进行分区。</p>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p><strong>警告类：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Alert</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String message;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Alert</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Alert</span><span class="params">(String message, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.message = message;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;(&quot;</span> + message + <span class="string">&quot;, &quot;</span> + timestamp + <span class="string">&quot;)&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>烟雾传感器读数类：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">SmokeLevel</span> </span>&#123;</span><br><span class="line">    LOW,</span><br><span class="line">    HIGH</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>产生烟雾传感器读数的自定义数据源：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SmokeLevelSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">SmokeLevel</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;SmokeLevel&gt; srcCtx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Random rand = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (rand.nextGaussian() &gt; <span class="number">0.8</span>) &#123;</span><br><span class="line">                srcCtx.collect(SmokeLevel.HIGH);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                srcCtx.collect(SmokeLevel.LOW);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultiStreamTransformations</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SensorReading&gt; tempReadings = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> SensorSource());</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SmokeLevel&gt; smokeReadings = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> SmokeLevelSource())</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;SensorReading, String&gt; keyedTempReadings = tempReadings</span><br><span class="line">                .keyBy(r -&gt; r.id);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Alert&gt; alerts = keyedTempReadings</span><br><span class="line">                .connect(smokeReadings.broadcast())</span><br><span class="line">                .flatMap(<span class="keyword">new</span> RaiseAlertFlatMap());</span><br><span class="line"></span><br><span class="line">        alerts.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;Multi-Stream Transformations Example&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RaiseAlertFlatMap</span> <span class="keyword">implements</span> <span class="title">CoFlatMapFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">SmokeLevel</span>, <span class="title">Alert</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> SmokeLevel smokeLevel = SmokeLevel.LOW;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap1</span><span class="params">(SensorReading tempReading, Collector&lt;Alert&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// high chance of fire =&gt; true</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.smokeLevel == SmokeLevel.HIGH &amp;&amp; tempReading.temperature &gt; <span class="number">100</span>) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Alert(<span class="string">&quot;Risk of fire! &quot;</span> + tempReading, tempReading.timestamp));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap2</span><span class="params">(SmokeLevel smokeLevel, Collector&lt;Alert&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// update smoke level</span></span><br><span class="line">            <span class="keyword">this</span>.smokeLevel = smokeLevel;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume日志采集</title>
    <url>/2021/01/17/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/</url>
    <content><![CDATA[<p>Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。其在日志文件进行采集过程中起到了很重要的作用，下面将对于进行详细介绍，并进行案例介绍！</p>
<a id="more"></a>

<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h2 id="Flume-介绍"><a href="#Flume-介绍" class="headerlink" title="Flume 介绍"></a>Flume 介绍</h2><p>​        Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。<br>Apache Flume的使用不仅仅局限于日志数据聚合。由于数据源是可定制的，Flume可以用于传输大量事件数据，包括但不限于网络流量数据、社交媒体生成的数据、电子邮件消息和几乎所有可能的数据源。</p>
<p><strong>Flume的特点：</strong> </p>
<p>（1）可以和任意集中式存储进行集成（HDFS，HBASE）</p>
<p>（2）输入的数据速率大于写入存储目的地速率，flume会进行缓冲</p>
<p>（3）flume提供上下文路由（数据流路线）</p>
<p>（4）   Flume中的事物基于channel，使用了两个事物模型（sender+receiver）,确保消息被可靠发送</p>
<p>（5）   flume是 可靠的，容错的，可扩展的。</p>
<p>（6）   flume易实现只需要简单的配置数据源以及存储端的相关信息即可，不需要写复杂的代码！！！</p>
<h2 id="Flume组件"><a href="#Flume组件" class="headerlink" title="Flume组件"></a>Flume组件</h2><p>Flume 实现日志文件的采集，其主要是不断地去监听日志文件的变化，最后将日志文件中变化的数据流采集到HDFS中。如下图所示，Flume主要由3个部分组成：</p>
<ul>
<li>Event：消息的基本单位，有header和body组成</li>
<li>Agent：JVM进程，负责将一端外部来源产生的消息转 发到另一端外部的目的地<ul>
<li>Source：从外部来源读入event，并写入channel</li>
<li>Channel：event暂存组件，source写入后，event将会 一直保存,</li>
<li>Sink：从channel读入event，并写入目的地</li>
</ul>
</li>
</ul>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225816353-596309472.png">    </p>
<p>那Flume到底是如何工作的呢？主要还是通过上面原型图将源源不断地数据流存储到外部，对于一条数据也就是事件流如下图所示过程实现采集。我们把一次source-channel-sink称为是一个agent。 </p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225821153-557297293.png">  </p>
<p>  对于数据流，则是将不断产生的事件流最后进行汇总即可，如下所示：</p>
<p><img src="img/flume3.png">  </p>
<h1 id="Flume实践"><a href="#Flume实践" class="headerlink" title="Flume实践"></a>Flume实践</h1><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>Flume的实现则是通过配置conf文件即可完成数据的采集。如下所示，是一个flume配置文件。</p>
<p>Flume采集到HDFS - 使用exec source</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> a2 则为 这次 Flume 任务agent的名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> r2 则为 <span class="built_in">source</span> 名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> k2 - sink 名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> c2 - channel 名称</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>的信息</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">type</span> - <span class="built_in">source</span> 类型</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> - 监听命令 监听的文件 XXX/XXX.log</span> </span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line">a2.sources.r2.command = tail -F XXX/XXX.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置sink信息</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">type</span> - 流出存储地</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hdfs.path - hdfs 地址 数据按时间分区ds进行存放</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rollInterval 按时间生成 HDFS 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rollCount：按写入的 event 的个数触发 roll，生成新的 HDFS 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fileType：指定保存到 HDFS 文件系统上的文件类型</span></span><br><span class="line"></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://XXXXhdfsIP和端口/数据存储地址</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 0</span><br><span class="line">a2.sinks.k2.hdfs.tollSize = 1024000</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 时间拦截</span> </span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line">a2.sinks.k2.hdfs.callTimeout = 60000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel  <span class="built_in">type</span> 以及 存储信息</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将sink和<span class="built_in">source</span>通过channel 连接起来</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Source-参数说明"><a href="#Source-参数说明" class="headerlink" title="Source 参数说明"></a>Source 参数说明</h3><p>对接各种外部数据源，将收集到的事件发送到Channel中，一个source可以向多个channel发送event，Flume内置非常丰富的Source，同时用户可以自定义Source。针对不同的Source来不同的参数配置，下面给出了一些常用参数。</p>
<table>
<thead>
<tr>
<th align="center"><strong>Source类型</strong></th>
<th align="center"><strong>Type</strong></th>
<th align="center"><strong>用途</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">Avro Source</td>
<td align="center">avro</td>
<td align="center">启动一个Avro Server，可与上一级Agent连接</td>
</tr>
<tr>
<td align="center">Exec Source</td>
<td align="center">exec</td>
<td align="center">执行unix command，获取标准输出，如tail -f</td>
</tr>
<tr>
<td align="center">Taildir Source</td>
<td align="center">TAILDIR</td>
<td align="center">监听目录或文件</td>
</tr>
<tr>
<td align="center">Spooling Directory Source</td>
<td align="center">spooldir</td>
<td align="center">监听目录下的新增文件</td>
</tr>
<tr>
<td align="center">Kafka Source</td>
<td align="center">org.apache.flume.sourc e.kafka.KafkaSource</td>
<td align="center">读取Kafka数据</td>
</tr>
<tr>
<td align="center">JMS Source</td>
<td align="center">jms</td>
<td align="center">从JMS源读取数据</td>
</tr>
</tbody></table>
<p><strong>NetCat Source</strong></p>
<ul>
<li>NetCat Source可以使用TCP和UDP两种协议方式，使用方法基本相同，通过监听指定的IP和端口来传输数据，它会将监听到的每一行数据转化成一个Event写入到Channel中。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name   Default       Description</span><br><span class="line">channels@          –     </span><br><span class="line">type@              –          类型指定为：netcat</span><br><span class="line">bind@              –          绑定机器名或IP地址</span><br><span class="line">port@              –          端口号</span><br><span class="line">max-line-length   512         一行的最大字节数</span><br><span class="line">ack-every-event   true        对成功接受的Event返回OK</span><br><span class="line">selector.type   replicating   选择器类型replicating or multiplexing</span><br><span class="line">selector.*                    选择器相关参数</span><br><span class="line">interceptors       –          拦截器列表，多个以空格分隔</span><br><span class="line">interceptors.*                拦截器相关参数</span><br></pre></td></tr></table></figure>
<p><strong>Avro Source</strong></p>
<ul>
<li>不同主机上的Agent通过网络传输数据可使用的Source，一般是接受Avro client的数据，或和是上一级Agent的Avro Sink成对存在。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Property Name              Default          Description</span><br><span class="line">channels@                    –   </span><br><span class="line">type@                        –              类型指定为：avro</span><br><span class="line">bind@                        –              监听的主机名或IP地址</span><br><span class="line">port@                        –              端口号</span><br><span class="line">threads                      –              传输可使用的最大线程数</span><br><span class="line">selector.type        </span><br><span class="line">selector.*       </span><br><span class="line">interceptors                 –              拦截器列表</span><br><span class="line">interceptors.*       </span><br><span class="line">compression-type            none            可设置为“none” 或 “deflate”. 压缩类型需要和AvroSource匹配</span><br></pre></td></tr></table></figure>
<p><strong>Exec Source</strong>   </p>
<ul>
<li>Exec source通过执行给定的Unix命令的传输结果数据，如，cat，tail -F等，实时性比较高，但是一旦Agent进程出现问题，可能会导致数据的丢失。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name            Default                 Description</span><br><span class="line">channels@                   –    </span><br><span class="line">type@                       –                    类型指定为：exec</span><br><span class="line">command@                    –                    需要去执行的命令</span><br><span class="line">shell                       –                    运行命令的shell脚本文件</span><br><span class="line">restartThrottle           10000                  尝试重启的超时时间</span><br><span class="line">restart                   false                  如果命令执行失败，是否重启</span><br><span class="line">logStdErr                 false                  是否记录错误日志</span><br><span class="line">batchSize                  20                    批次写入channel的最大日志数量</span><br><span class="line">batchTimeout              3000                   批次写入数据的最大等待时间（毫秒）</span><br><span class="line">selector.type          replicating               选择器类型replicating or multiplexing</span><br><span class="line">selector.*                                       选择器其他参数</span><br><span class="line">interceptors                –                    拦截器列表，多个空格分隔</span><br><span class="line">interceptors.*       </span><br></pre></td></tr></table></figure>
<p><strong>Spooling Directory Source</strong></p>
<ul>
<li>通过监控一个文件夹将新增文件内容转换成Event传输数据，特点是不会丢失数据，使用Spooling Directory Source需要注意的两点是，1)不能对被监控的文件夹下的新增的文件做出任何更改，2）新增到监控文件夹的文件名称必须是唯一的。由于是对整个新增文件的监控，Spooling Directory Source的实时性相对较低，不过可以采用对文件高粒度分割达到近似实时。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name                 Default             Description</span><br><span class="line">channels@                        –   </span><br><span class="line">type@                            –                类型指定：spooldir.</span><br><span class="line">spoolDir@                        –                被监控的文件夹目录</span><br><span class="line">fileSuffix                  .COMPLETED            完成数据传输的文件后缀标志</span><br><span class="line">deletePolicy                   never              删除已经完成数据传输的文件时间：never or immediate</span><br><span class="line">fileHeader                     false              是否在header中添加文件的完整路径信息</span><br><span class="line">fileHeaderKey                   file              如果header中添加文件的完整路径信息时key的名称</span><br><span class="line">basenameHeader                 false              是否在header中添加文件的基本名称信息</span><br><span class="line">basenameHeaderKey             basename            如果header中添加文件的基本名称信息时key的名称</span><br><span class="line">includePattern                 ^.*$               使用正则来匹配新增文件需要被传输数据的文件</span><br><span class="line">ignorePattern                   ^$                使用正则来忽略新增的文件</span><br><span class="line">trackerDir                  .flumespool           存储元数据信息目录</span><br><span class="line">consumeOrder                  oldest              文件消费顺序：oldest, youngest and random.</span><br><span class="line">maxBackoff                     4000               如果channel容量不足，尝试写入的超时时间，如果仍然不能写入，则会抛出ChannelException</span><br><span class="line">batchSize                      100                批次处理粒度</span><br><span class="line">inputCharset                  UTF-8               输入码表格式</span><br><span class="line">decodeErrorPolicy              FAIL               遇到不可解码字符后的处理方式：FAIL，REPLACE，IGNORE</span><br><span class="line">selector.type               replicating           选择器类型：replicating or multiplexing</span><br><span class="line">selector.*                                        选择器其他参数</span><br><span class="line">interceptors                    –                 拦截器列表，空格分隔</span><br><span class="line">interceptors.*  </span><br></pre></td></tr></table></figure>
<p><strong>Taildir Source</strong></p>
<ul>
<li>可以实时的监控指定一个或多个文件中的新增内容，由于该方式将数据的偏移量保存在一个指定的json文件中，即使在Agent挂掉或被kill也不会有数据的丢失，需要注意的是，该Source不能在Windows上使用。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name                      Default            Description</span><br><span class="line">channels@                             –  </span><br><span class="line">type@                                 –               指定类型：TAILDIR.</span><br><span class="line">filegroups@                           –               文件组的名称，多个空格分隔</span><br><span class="line">filegroups.&lt;filegroupName&gt;@           –               被监控文件的绝对路径</span><br><span class="line">positionFile         ~/.flume/taildir_position.json      存储数据偏移量路径</span><br><span class="line">headers.&lt;filegroupName&gt;.&lt;headerKey&gt;   –               Header key的名称</span><br><span class="line">byteOffsetHeader                    false             是否添加字节偏移量到key为‘byteoffset’值中</span><br><span class="line">skipToEnd                           false             当偏移量不能写入到文件时是否跳到文件结尾</span><br><span class="line">idleTimeout                         120000            关闭没有新增内容的文件超时时间（毫秒）</span><br><span class="line">writePosInterval                    3000              在positionfile 写入每一个文件lastposition的时间间隔</span><br><span class="line">batchSize                            100              批次处理行数</span><br><span class="line">fileHeader                          false             是否添加header存储文件绝对路径</span><br><span class="line">fileHeaderKey                       file              fileHeader启用时，使用的key</span><br></pre></td></tr></table></figure>
<h3 id="Channels参数说明"><a href="#Channels参数说明" class="headerlink" title="Channels参数说明"></a>Channels参数说明</h3><p><strong>Memory Channel</strong></p>
<ul>
<li>Memory Channel是使用内存来存储Event，使用内存的意味着数据传输速率会很快，但是当Agent挂掉后，存储在Channel中的数据将会丢失。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name                 Default                Description</span><br><span class="line">type@                            –                   类型指定为：memory</span><br><span class="line">capacity                        100                  存储在channel中的最大容量</span><br><span class="line">transactionCapacity             100                  从一个source中去或者给一个sink，每个事务中最大的事件数</span><br><span class="line">keep-alive                       3                   对于添加或者删除一个事件的超时的秒钟</span><br><span class="line">byteCapacityBufferPercentage    20                   定义缓存百分比</span><br><span class="line">byteCapacity              see description            Channel中允许存储的最大字节总数</span><br></pre></td></tr></table></figure>
<p><strong>File Channel</strong></p>
<ul>
<li>File Channel使用磁盘来存储Event，速率相对于Memory Channel较慢，但数据不会丢失</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Property Name            Default                  Description    </span><br><span class="line">type@                      –                      类型指定：file.</span><br><span class="line">checkpointDir   ~/.flume/file-channel/checkpoint  checkpoint目录</span><br><span class="line">useDualCheckpoints       false                    备份checkpoint，为True，backupCheckpointDir必须设置</span><br><span class="line">backupCheckpointDir        –                      备份checkpoint目录</span><br><span class="line">dataDirs    ~/.flume/file-channel/data           数据存储所在的目录设置</span><br><span class="line">transactionCapacity      10000                    Event存储最大值</span><br><span class="line">checkpointInterval       30000                    checkpoint间隔时间</span><br><span class="line">maxFileSize            2146435071                 单一日志最大设置字节数</span><br><span class="line">minimumRequiredSpace    524288000                 最小的请求闲置空间（以字节为单位）</span><br><span class="line">capacity                 1000000                  Channel最大容量</span><br><span class="line">keep-alive                 3                      一个存放操作的等待时间值（秒）</span><br><span class="line">use-log-replay-v1         false                   Expert: 使用老的回复逻辑</span><br><span class="line">use-fast-replay           false                   Expert: 回复不需要队列</span><br><span class="line">checkpointOnClose         true         </span><br></pre></td></tr></table></figure>
<h3 id="Sink参数说明"><a href="#Sink参数说明" class="headerlink" title="Sink参数说明"></a>Sink参数说明</h3><p>Flume常用Sinks有Log Sink，HDFS Sink，Avro Sink，Kafka Sink，当然也可以自定义Sink。</p>
<p><strong>Logger Sink</strong> </p>
<ul>
<li>Logger Sink以INFO 级别的日志记录到log日志中，这种方式通常用于测试。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Property Name          Default           Description</span><br><span class="line">channel@                 –   </span><br><span class="line">type＠                   –               类型指定：logger</span><br><span class="line">maxBytesToLog           16               能够记录的最大Event Body字节数  </span><br></pre></td></tr></table></figure>
<p><strong>HDFS Sink</strong> </p>
<ul>
<li>Sink数据到HDFS，目前支持text 和 sequence files两种文件格式，支持压缩，并可以对数据进行分区，分桶存储。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Name                   Default               Description</span><br><span class="line">channel@                  –  </span><br><span class="line">type@                     –                  指定类型：hdfs</span><br><span class="line">hdfs.path@                –                  HDFS的路径，eg hdfs:&#x2F;&#x2F;namenode&#x2F;flume&#x2F;webdata&#x2F;</span><br><span class="line">hdfs.filePrefix        FlumeData             保存数据文件的前缀名</span><br><span class="line">hdfs.fileSuffix           –                  保存数据文件的后缀名</span><br><span class="line">hdfs.inUsePrefix          –                  临时写入的文件前缀名</span><br><span class="line">hdfs.inUseSuffix         .tmp                临时写入的文件后缀名</span><br><span class="line">hdfs.rollInterval         30                 间隔多长将临时文件滚动成最终目标文件，单位：秒，</span><br><span class="line">                                             如果设置成0，则表示不根据时间来滚动文件</span><br><span class="line">hdfs.rollSize            1024                当临时文件达到多少（单位：bytes）时，滚动成目标文件，</span><br><span class="line">                                             如果设置成0，则表示不根据临时文件大小来滚动文件</span><br><span class="line">hdfs.rollCount            10                 当 events 数据达到该数量时候，将临时文件滚动成目标文件，</span><br><span class="line">                                             如果设置成0，则表示不根据events数据来滚动文件</span><br><span class="line">hdfs.idleTimeout          0                  当目前被打开的临时文件在该参数指定的时间（秒）内，</span><br><span class="line">                                             没有任何数据写入，则将该临时文件关闭并重命名成目标文件</span><br><span class="line">hdfs.batchSize           100                 每个批次刷新到 HDFS 上的 events 数量</span><br><span class="line">hdfs.codeC                –                  文件压缩格式，包括：gzip, bzip2, lzo, lzop, snappy</span><br><span class="line">hdfs.fileType         SequenceFile           文件格式，包括：SequenceFile, DataStream,CompressedStre，</span><br><span class="line">                                             当使用DataStream时候，文件不会被压缩，不需要设置hdfs.codeC;</span><br><span class="line">                                             当使用CompressedStream时候，必须设置一个正确的hdfs.codeC值；</span><br><span class="line">hdfs.maxOpenFiles        5000                最大允许打开的HDFS文件数，当打开的文件数达到该值，</span><br><span class="line">                                             最早打开的文件将会被关闭</span><br><span class="line">hdfs.minBlockReplicas     –                  HDFS副本数，写入 HDFS 文件块的最小副本数。</span><br><span class="line">                                             该参数会影响文件的滚动配置，一般将该参数配置成1，才可以按照配置正确滚动文件</span><br><span class="line">hdfs.writeFormat        Writable             写 sequence 文件的格式。包含：Text, Writable（默认）</span><br><span class="line">hdfs.callTimeout         10000               执行HDFS操作的超时时间（单位：毫秒）</span><br><span class="line">hdfs.threadsPoolSize      10                 hdfs sink 启动的操作HDFS的线程数</span><br><span class="line">hdfs.rollTimerPoolSize    1                  hdfs sink 启动的根据时间滚动文件的线程数</span><br><span class="line">hdfs.kerberosPrincipal    –                  HDFS安全认证kerberos配置</span><br><span class="line">hdfs.kerberosKeytab       –                  HDFS安全认证kerberos配置</span><br><span class="line">hdfs.proxyUser                               代理用户</span><br><span class="line">hdfs.round              false                是否启用时间上的”舍弃”</span><br><span class="line">hdfs.roundValue           1                  时间上进行“舍弃”的值</span><br><span class="line">hdfs.roundUnit          second               时间上进行”舍弃”的单位，包含：second,minute,hour</span><br><span class="line">hdfs.timeZone         Local Time             时区。</span><br><span class="line">hdfs.useLocalTimeStamp  false                是否使用当地时间</span><br><span class="line">hdfs.closeTries 0       Number               hdfs sink 关闭文件的尝试次数；</span><br><span class="line">                                             如果设置为1，当一次关闭文件失败后，hdfs sink将不会再次尝试关闭文件，</span><br><span class="line">                                             这个未关闭的文件将会一直留在那，并且是打开状态；</span><br><span class="line">                                             设置为0，当一次关闭失败后，hdfs sink会继续尝试下一次关闭，直到成功</span><br><span class="line">hdfs.retryInterval        180                hdfs sink 尝试关闭文件的时间间隔，</span><br><span class="line">                                             如果设置为0，表示不尝试，相当于于将hdfs.closeTries设置成1</span><br><span class="line">serializer               TEXT                序列化类型</span><br><span class="line">serializer.*                 </span><br></pre></td></tr></table></figure>
<p><strong>Avro Sink</strong> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Property Name              Default              Description</span><br><span class="line">channel@                         –   </span><br><span class="line">type@                        –                  指定类型：avro.</span><br><span class="line">hostname@                    –                  主机名或IP</span><br><span class="line">port@                        –                  端口号</span><br><span class="line">batch-size                  100                 批次处理Event数</span><br><span class="line">connect-timeout            20000                连接超时时间</span><br><span class="line">request-timeout            20000                请求超时时间</span><br><span class="line">compression-type            none                压缩类型，“none” or “deflate”.</span><br><span class="line">compression-level            6                  压缩级别，0表示不压缩，1-9数字越大，压缩比越高</span><br><span class="line">ssl                        false                使用ssl加密</span><br></pre></td></tr></table></figure>
<p><strong>Kafka Sink</strong></p>
<ul>
<li>传输数据到Kafka中，需要注意的是Flume版本和Kafka版本的兼容性</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Property Name              Default             Description</span><br><span class="line">type                         –                 指定类型：org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">kafka.bootstrap.servers      –                 kafka服务地址</span><br><span class="line">kafka.topic          default-flume-topic       kafka Topic</span><br><span class="line">flumeBatchSize              100                批次写入kafka Event数</span><br><span class="line">kafka.producer.acks          1                 多少个副本确认后才能确定消息传递成功，0表示不需要确认</span><br><span class="line">                                               1表示只需要首要的副本得到确认，-1表示等待所有确认。</span><br></pre></td></tr></table></figure>
<h2 id="运行Flume"><a href="#运行Flume" class="headerlink" title="运行Flume"></a>运行Flume</h2><p>执行如下命令来开始Flume任务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent --conf flume-conf 配置路径 --conf-file 采集任务配置文件 -name agent名 -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure>
<p>如下图所示，说明Flume开始了采集任务，</p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225931033-1937068728.png">  </p>
<p>如下图，是通过jar包产生的一组测试数据，当flume开始工作的时候，会不断输出如下所示数据流信息，同时将采集到数据存储到hdfs路径下，后面的后拽则是相应在HDFS中的文件名。</p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225940383-582032821.png"></p>
<p>   对于flume任务来说，在设定时间限制的情况下，不需要专门的去停止flume任务，否则利用ctrl+z强制停止，然后再使用kill 命令去kill掉进程，<code>ps -ef | grep XXX.conf</code> 利用该命令去查询相应的进程 然后kill即可。</p>
<p>​    主要是第一次使用Flume工具，因此，有很多的不懂，整个过程虽然简单，但是还是磕磕绊绊吧。下面篇我们利用Java来产生大量日志数据（继而调用相应的jar包  并通过flume监听 采集到HDFS中）。</p>
]]></content>
      <categories>
        <category>大数据学习</category>
        <category>日志采集</category>
      </categories>
      <tags>
        <tag>flume</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
</search>
