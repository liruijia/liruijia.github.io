<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liruijia.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。其在日志文件进行采集过程中起到了很重要的作用，下面将对于进行详细介绍，并进行案例介绍！">
<meta property="og:type" content="article">
<meta property="og:title" content="Flume日志采集">
<meta property="og:url" content="http://liruijia.github.io/2021/01/17/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/index.html">
<meta property="og:site_name" content="Liruijia&#39;s Blog">
<meta property="og:description" content="Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。其在日志文件进行采集过程中起到了很重要的作用，下面将对于进行详细介绍，并进行案例介绍！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225816353-596309472.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225821153-557297293.png">
<meta property="og:image" content="http://liruijia.github.io/.io//flume3.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225931033-1937068728.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225940383-582032821.png">
<meta property="article:published_time" content="2021-01-17T13:11:15.000Z">
<meta property="article:modified_time" content="2021-01-22T15:13:37.803Z">
<meta property="article:author" content="Liruijia">
<meta property="article:tag" content="flume">
<meta property="article:tag" content="HDFS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225816353-596309472.png">


<link rel="canonical" href="http://liruijia.github.io/2021/01/17/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Flume日志采集 | Liruijia's Blog</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Liruijia's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Liruijia's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Hello world</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-friends"><a href="/friends/" rel="section"><i class="fab fa-weixin fa-fw"></i>friends</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Flume"><span class="nav-number">1.</span> <span class="nav-text">Flume</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Flume-%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.</span> <span class="nav-text">Flume 介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flume%E7%BB%84%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">Flume组件</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flume%E5%AE%9E%E8%B7%B5"><span class="nav-number">2.</span> <span class="nav-text">Flume实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">配置文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Source-%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.1.</span> <span class="nav-text">Source 参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Channels%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.2.</span> <span class="nav-text">Channels参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sink%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.3.</span> <span class="nav-text">Sink参数说明</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8CFlume"><span class="nav-number">2.2.</span> <span class="nav-text">运行Flume</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liruijia"
      src="/images/3.jpg">
  <p class="site-author-name" itemprop="name">Liruijia</p>
  <div class="site-description" itemprop="description">记录一次一次的爬坑经历！！！</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/liruijia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liruijia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2406891860@qq.com" title="E-Mail → mailto:2406891860@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.cnblogs.com/learn-ruijiali/" title="Cnblogs → https:&#x2F;&#x2F;www.cnblogs.com&#x2F;learn-ruijiali&#x2F;" rel="noopener" target="_blank"><i class="fab fa-blogs fa-fw"></i>Cnblogs</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://liruijia.github.io/2021/01/17/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/3.jpg">
      <meta itemprop="name" content="Liruijia">
      <meta itemprop="description" content="记录一次一次的爬坑经历！！！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liruijia's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Flume日志采集
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-01-17 21:11:15" itemprop="dateCreated datePublished" datetime="2021-01-17T21:11:15+08:00">2021-01-17</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-01-22 23:13:37" itemprop="dateModified" datetime="2021-01-22T23:13:37+08:00">2021-01-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">大数据学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/" itemprop="url" rel="index"><span itemprop="name">日志采集</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。其在日志文件进行采集过程中起到了很重要的作用，下面将对于进行详细介绍，并进行案例介绍！</p>
<a id="more"></a>

<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h2 id="Flume-介绍"><a href="#Flume-介绍" class="headerlink" title="Flume 介绍"></a>Flume 介绍</h2><p>​        Flume是cloudera开发的后来贡献给了Apache的一套用分布式，高可靠的，高可用的海量分布式日志采集、聚合和传输的系统，将大量日志数据从许多不同的源移动到一个集中的数据存储。<br>Apache Flume的使用不仅仅局限于日志数据聚合。由于数据源是可定制的，Flume可以用于传输大量事件数据，包括但不限于网络流量数据、社交媒体生成的数据、电子邮件消息和几乎所有可能的数据源。</p>
<p><strong>Flume的特点：</strong> </p>
<p>（1）可以和任意集中式存储进行集成（HDFS，HBASE）</p>
<p>（2）输入的数据速率大于写入存储目的地速率，flume会进行缓冲</p>
<p>（3）flume提供上下文路由（数据流路线）</p>
<p>（4）   Flume中的事物基于channel，使用了两个事物模型（sender+receiver）,确保消息被可靠发送</p>
<p>（5）   flume是 可靠的，容错的，可扩展的。</p>
<p>（6）   flume易实现只需要简单的配置数据源以及存储端的相关信息即可，不需要写复杂的代码！！！</p>
<h2 id="Flume组件"><a href="#Flume组件" class="headerlink" title="Flume组件"></a>Flume组件</h2><p>Flume 实现日志文件的采集，其主要是不断地去监听日志文件的变化，最后将日志文件中变化的数据流采集到HDFS中。如下图所示，Flume主要由3个部分组成：</p>
<ul>
<li>Event：消息的基本单位，有header和body组成</li>
<li>Agent：JVM进程，负责将一端外部来源产生的消息转 发到另一端外部的目的地<ul>
<li>Source：从外部来源读入event，并写入channel</li>
<li>Channel：event暂存组件，source写入后，event将会 一直保存,</li>
<li>Sink：从channel读入event，并写入目的地</li>
</ul>
</li>
</ul>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225816353-596309472.png">    </p>
<p>那Flume到底是如何工作的呢？主要还是通过上面原型图将源源不断地数据流存储到外部，对于一条数据也就是事件流如下图所示过程实现采集。我们把一次source-channel-sink称为是一个agent。 </p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225821153-557297293.png">  </p>
<p>  对于数据流，则是将不断产生的事件流最后进行汇总即可，如下所示：</p>
<p><img src="/.io//flume3.png">  </p>
<h1 id="Flume实践"><a href="#Flume实践" class="headerlink" title="Flume实践"></a>Flume实践</h1><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>Flume的实现则是通过配置conf文件即可完成数据的采集。如下所示，是一个flume配置文件。</p>
<p>Flume采集到HDFS - 使用exec source</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> a2 则为 这次 Flume 任务agent的名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> r2 则为 <span class="built_in">source</span> 名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> k2 - sink 名称</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> c2 - channel 名称</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>的信息</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">type</span> - <span class="built_in">source</span> 类型</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> - 监听命令 监听的文件 XXX/XXX.log</span> </span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line">a2.sources.r2.command = tail -F XXX/XXX.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置sink信息</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">type</span> - 流出存储地</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hdfs.path - hdfs 地址 数据按时间分区ds进行存放</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rollInterval 按时间生成 HDFS 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rollCount：按写入的 event 的个数触发 roll，生成新的 HDFS 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fileType：指定保存到 HDFS 文件系统上的文件类型</span></span><br><span class="line"></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://XXXXhdfsIP和端口/数据存储地址</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 0</span><br><span class="line">a2.sinks.k2.hdfs.tollSize = 1024000</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 时间拦截</span> </span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line">a2.sinks.k2.hdfs.callTimeout = 60000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel  <span class="built_in">type</span> 以及 存储信息</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将sink和<span class="built_in">source</span>通过channel 连接起来</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Source-参数说明"><a href="#Source-参数说明" class="headerlink" title="Source 参数说明"></a>Source 参数说明</h3><p>对接各种外部数据源，将收集到的事件发送到Channel中，一个source可以向多个channel发送event，Flume内置非常丰富的Source，同时用户可以自定义Source。针对不同的Source来不同的参数配置，下面给出了一些常用参数。</p>
<table>
<thead>
<tr>
<th align="center"><strong>Source类型</strong></th>
<th align="center"><strong>Type</strong></th>
<th align="center"><strong>用途</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">Avro Source</td>
<td align="center">avro</td>
<td align="center">启动一个Avro Server，可与上一级Agent连接</td>
</tr>
<tr>
<td align="center">Exec Source</td>
<td align="center">exec</td>
<td align="center">执行unix command，获取标准输出，如tail -f</td>
</tr>
<tr>
<td align="center">Taildir Source</td>
<td align="center">TAILDIR</td>
<td align="center">监听目录或文件</td>
</tr>
<tr>
<td align="center">Spooling Directory Source</td>
<td align="center">spooldir</td>
<td align="center">监听目录下的新增文件</td>
</tr>
<tr>
<td align="center">Kafka Source</td>
<td align="center">org.apache.flume.sourc e.kafka.KafkaSource</td>
<td align="center">读取Kafka数据</td>
</tr>
<tr>
<td align="center">JMS Source</td>
<td align="center">jms</td>
<td align="center">从JMS源读取数据</td>
</tr>
</tbody></table>
<p><strong>NetCat Source</strong></p>
<ul>
<li>NetCat Source可以使用TCP和UDP两种协议方式，使用方法基本相同，通过监听指定的IP和端口来传输数据，它会将监听到的每一行数据转化成一个Event写入到Channel中。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Property Name   Default       Description</span><br><span class="line">channels@          –     </span><br><span class="line">type@              –          类型指定为：netcat</span><br><span class="line">bind@              –          绑定机器名或IP地址</span><br><span class="line">port@              –          端口号</span><br><span class="line">max-line-length   512         一行的最大字节数</span><br><span class="line">ack-every-event   true        对成功接受的Event返回OK</span><br><span class="line">selector.type   replicating   选择器类型replicating or multiplexing</span><br><span class="line">selector.*                    选择器相关参数</span><br><span class="line">interceptors       –          拦截器列表，多个以空格分隔</span><br><span class="line">interceptors.*                拦截器相关参数</span><br></pre></td></tr></table></figure>
<p><strong>Avro Source</strong></p>
<ul>
<li>不同主机上的Agent通过网络传输数据可使用的Source，一般是接受Avro client的数据，或和是上一级Agent的Avro Sink成对存在。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Property Name              Default          Description</span><br><span class="line">channels@                    –   </span><br><span class="line">type@                        –              类型指定为：avro</span><br><span class="line">bind@                        –              监听的主机名或IP地址</span><br><span class="line">port@                        –              端口号</span><br><span class="line">threads                      –              传输可使用的最大线程数</span><br><span class="line">selector.type        </span><br><span class="line">selector.*       </span><br><span class="line">interceptors                 –              拦截器列表</span><br><span class="line">interceptors.*       </span><br><span class="line">compression-type            none            可设置为“none” 或 “deflate”. 压缩类型需要和AvroSource匹配</span><br></pre></td></tr></table></figure>
<p><strong>Exec Source</strong>   </p>
<ul>
<li>Exec source通过执行给定的Unix命令的传输结果数据，如，cat，tail -F等，实时性比较高，但是一旦Agent进程出现问题，可能会导致数据的丢失。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Property Name            Default                 Description</span><br><span class="line">channels@                   –    </span><br><span class="line">type@                       –                    类型指定为：exec</span><br><span class="line">command@                    –                    需要去执行的命令</span><br><span class="line">shell                       –                    运行命令的shell脚本文件</span><br><span class="line">restartThrottle           10000                  尝试重启的超时时间</span><br><span class="line">restart                   false                  如果命令执行失败，是否重启</span><br><span class="line">logStdErr                 false                  是否记录错误日志</span><br><span class="line">batchSize                  20                    批次写入channel的最大日志数量</span><br><span class="line">batchTimeout              3000                   批次写入数据的最大等待时间（毫秒）</span><br><span class="line">selector.type          replicating               选择器类型replicating or multiplexing</span><br><span class="line">selector.*                                       选择器其他参数</span><br><span class="line">interceptors                –                    拦截器列表，多个空格分隔</span><br><span class="line">interceptors.*       </span><br></pre></td></tr></table></figure>
<p><strong>Spooling Directory Source</strong></p>
<ul>
<li>通过监控一个文件夹将新增文件内容转换成Event传输数据，特点是不会丢失数据，使用Spooling Directory Source需要注意的两点是，1)不能对被监控的文件夹下的新增的文件做出任何更改，2）新增到监控文件夹的文件名称必须是唯一的。由于是对整个新增文件的监控，Spooling Directory Source的实时性相对较低，不过可以采用对文件高粒度分割达到近似实时。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Property Name                 Default             Description</span><br><span class="line">channels@                        –   </span><br><span class="line">type@                            –                类型指定：spooldir.</span><br><span class="line">spoolDir@                        –                被监控的文件夹目录</span><br><span class="line">fileSuffix                  .COMPLETED            完成数据传输的文件后缀标志</span><br><span class="line">deletePolicy                   never              删除已经完成数据传输的文件时间：never or immediate</span><br><span class="line">fileHeader                     false              是否在header中添加文件的完整路径信息</span><br><span class="line">fileHeaderKey                   file              如果header中添加文件的完整路径信息时key的名称</span><br><span class="line">basenameHeader                 false              是否在header中添加文件的基本名称信息</span><br><span class="line">basenameHeaderKey             basename            如果header中添加文件的基本名称信息时key的名称</span><br><span class="line">includePattern                 ^.*$               使用正则来匹配新增文件需要被传输数据的文件</span><br><span class="line">ignorePattern                   ^$                使用正则来忽略新增的文件</span><br><span class="line">trackerDir                  .flumespool           存储元数据信息目录</span><br><span class="line">consumeOrder                  oldest              文件消费顺序：oldest, youngest and random.</span><br><span class="line">maxBackoff                     4000               如果channel容量不足，尝试写入的超时时间，如果仍然不能写入，则会抛出ChannelException</span><br><span class="line">batchSize                      100                批次处理粒度</span><br><span class="line">inputCharset                  UTF-8               输入码表格式</span><br><span class="line">decodeErrorPolicy              FAIL               遇到不可解码字符后的处理方式：FAIL，REPLACE，IGNORE</span><br><span class="line">selector.type               replicating           选择器类型：replicating or multiplexing</span><br><span class="line">selector.*                                        选择器其他参数</span><br><span class="line">interceptors                    –                 拦截器列表，空格分隔</span><br><span class="line">interceptors.*  </span><br></pre></td></tr></table></figure>
<p><strong>Taildir Source</strong></p>
<ul>
<li>可以实时的监控指定一个或多个文件中的新增内容，由于该方式将数据的偏移量保存在一个指定的json文件中，即使在Agent挂掉或被kill也不会有数据的丢失，需要注意的是，该Source不能在Windows上使用。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Property Name                      Default            Description</span><br><span class="line">channels@                             –  </span><br><span class="line">type@                                 –               指定类型：TAILDIR.</span><br><span class="line">filegroups@                           –               文件组的名称，多个空格分隔</span><br><span class="line">filegroups.&lt;filegroupName&gt;@           –               被监控文件的绝对路径</span><br><span class="line">positionFile         ~/.flume/taildir_position.json      存储数据偏移量路径</span><br><span class="line">headers.&lt;filegroupName&gt;.&lt;headerKey&gt;   –               Header key的名称</span><br><span class="line">byteOffsetHeader                    false             是否添加字节偏移量到key为‘byteoffset’值中</span><br><span class="line">skipToEnd                           false             当偏移量不能写入到文件时是否跳到文件结尾</span><br><span class="line">idleTimeout                         120000            关闭没有新增内容的文件超时时间（毫秒）</span><br><span class="line">writePosInterval                    3000              在positionfile 写入每一个文件lastposition的时间间隔</span><br><span class="line">batchSize                            100              批次处理行数</span><br><span class="line">fileHeader                          false             是否添加header存储文件绝对路径</span><br><span class="line">fileHeaderKey                       file              fileHeader启用时，使用的key</span><br></pre></td></tr></table></figure>
<h3 id="Channels参数说明"><a href="#Channels参数说明" class="headerlink" title="Channels参数说明"></a>Channels参数说明</h3><p><strong>Memory Channel</strong></p>
<ul>
<li>Memory Channel是使用内存来存储Event，使用内存的意味着数据传输速率会很快，但是当Agent挂掉后，存储在Channel中的数据将会丢失。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Property Name                 Default                Description</span><br><span class="line">type@                            –                   类型指定为：memory</span><br><span class="line">capacity                        100                  存储在channel中的最大容量</span><br><span class="line">transactionCapacity             100                  从一个source中去或者给一个sink，每个事务中最大的事件数</span><br><span class="line">keep-alive                       3                   对于添加或者删除一个事件的超时的秒钟</span><br><span class="line">byteCapacityBufferPercentage    20                   定义缓存百分比</span><br><span class="line">byteCapacity              see description            Channel中允许存储的最大字节总数</span><br></pre></td></tr></table></figure>
<p><strong>File Channel</strong></p>
<ul>
<li>File Channel使用磁盘来存储Event，速率相对于Memory Channel较慢，但数据不会丢失</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Property Name            Default                  Description    </span><br><span class="line">type@                      –                      类型指定：file.</span><br><span class="line">checkpointDir   ~/.flume/file-channel/checkpoint  checkpoint目录</span><br><span class="line">useDualCheckpoints       false                    备份checkpoint，为True，backupCheckpointDir必须设置</span><br><span class="line">backupCheckpointDir        –                      备份checkpoint目录</span><br><span class="line">dataDirs    ~/.flume/file-channel/data           数据存储所在的目录设置</span><br><span class="line">transactionCapacity      10000                    Event存储最大值</span><br><span class="line">checkpointInterval       30000                    checkpoint间隔时间</span><br><span class="line">maxFileSize            2146435071                 单一日志最大设置字节数</span><br><span class="line">minimumRequiredSpace    524288000                 最小的请求闲置空间（以字节为单位）</span><br><span class="line">capacity                 1000000                  Channel最大容量</span><br><span class="line">keep-alive                 3                      一个存放操作的等待时间值（秒）</span><br><span class="line">use-log-replay-v1         false                   Expert: 使用老的回复逻辑</span><br><span class="line">use-fast-replay           false                   Expert: 回复不需要队列</span><br><span class="line">checkpointOnClose         true         </span><br></pre></td></tr></table></figure>
<h3 id="Sink参数说明"><a href="#Sink参数说明" class="headerlink" title="Sink参数说明"></a>Sink参数说明</h3><p>Flume常用Sinks有Log Sink，HDFS Sink，Avro Sink，Kafka Sink，当然也可以自定义Sink。</p>
<p><strong>Logger Sink</strong> </p>
<ul>
<li>Logger Sink以INFO 级别的日志记录到log日志中，这种方式通常用于测试。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Property Name          Default           Description</span><br><span class="line">channel@                 –   </span><br><span class="line">type＠                   –               类型指定：logger</span><br><span class="line">maxBytesToLog           16               能够记录的最大Event Body字节数  </span><br></pre></td></tr></table></figure>
<p><strong>HDFS Sink</strong> </p>
<ul>
<li>Sink数据到HDFS，目前支持text 和 sequence files两种文件格式，支持压缩，并可以对数据进行分区，分桶存储。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">Name                   Default               Description</span><br><span class="line">channel@                  –  </span><br><span class="line">type@                     –                  指定类型：hdfs</span><br><span class="line">hdfs.path@                –                  HDFS的路径，eg hdfs:&#x2F;&#x2F;namenode&#x2F;flume&#x2F;webdata&#x2F;</span><br><span class="line">hdfs.filePrefix        FlumeData             保存数据文件的前缀名</span><br><span class="line">hdfs.fileSuffix           –                  保存数据文件的后缀名</span><br><span class="line">hdfs.inUsePrefix          –                  临时写入的文件前缀名</span><br><span class="line">hdfs.inUseSuffix         .tmp                临时写入的文件后缀名</span><br><span class="line">hdfs.rollInterval         30                 间隔多长将临时文件滚动成最终目标文件，单位：秒，</span><br><span class="line">                                             如果设置成0，则表示不根据时间来滚动文件</span><br><span class="line">hdfs.rollSize            1024                当临时文件达到多少（单位：bytes）时，滚动成目标文件，</span><br><span class="line">                                             如果设置成0，则表示不根据临时文件大小来滚动文件</span><br><span class="line">hdfs.rollCount            10                 当 events 数据达到该数量时候，将临时文件滚动成目标文件，</span><br><span class="line">                                             如果设置成0，则表示不根据events数据来滚动文件</span><br><span class="line">hdfs.idleTimeout          0                  当目前被打开的临时文件在该参数指定的时间（秒）内，</span><br><span class="line">                                             没有任何数据写入，则将该临时文件关闭并重命名成目标文件</span><br><span class="line">hdfs.batchSize           100                 每个批次刷新到 HDFS 上的 events 数量</span><br><span class="line">hdfs.codeC                –                  文件压缩格式，包括：gzip, bzip2, lzo, lzop, snappy</span><br><span class="line">hdfs.fileType         SequenceFile           文件格式，包括：SequenceFile, DataStream,CompressedStre，</span><br><span class="line">                                             当使用DataStream时候，文件不会被压缩，不需要设置hdfs.codeC;</span><br><span class="line">                                             当使用CompressedStream时候，必须设置一个正确的hdfs.codeC值；</span><br><span class="line">hdfs.maxOpenFiles        5000                最大允许打开的HDFS文件数，当打开的文件数达到该值，</span><br><span class="line">                                             最早打开的文件将会被关闭</span><br><span class="line">hdfs.minBlockReplicas     –                  HDFS副本数，写入 HDFS 文件块的最小副本数。</span><br><span class="line">                                             该参数会影响文件的滚动配置，一般将该参数配置成1，才可以按照配置正确滚动文件</span><br><span class="line">hdfs.writeFormat        Writable             写 sequence 文件的格式。包含：Text, Writable（默认）</span><br><span class="line">hdfs.callTimeout         10000               执行HDFS操作的超时时间（单位：毫秒）</span><br><span class="line">hdfs.threadsPoolSize      10                 hdfs sink 启动的操作HDFS的线程数</span><br><span class="line">hdfs.rollTimerPoolSize    1                  hdfs sink 启动的根据时间滚动文件的线程数</span><br><span class="line">hdfs.kerberosPrincipal    –                  HDFS安全认证kerberos配置</span><br><span class="line">hdfs.kerberosKeytab       –                  HDFS安全认证kerberos配置</span><br><span class="line">hdfs.proxyUser                               代理用户</span><br><span class="line">hdfs.round              false                是否启用时间上的”舍弃”</span><br><span class="line">hdfs.roundValue           1                  时间上进行“舍弃”的值</span><br><span class="line">hdfs.roundUnit          second               时间上进行”舍弃”的单位，包含：second,minute,hour</span><br><span class="line">hdfs.timeZone         Local Time             时区。</span><br><span class="line">hdfs.useLocalTimeStamp  false                是否使用当地时间</span><br><span class="line">hdfs.closeTries 0       Number               hdfs sink 关闭文件的尝试次数；</span><br><span class="line">                                             如果设置为1，当一次关闭文件失败后，hdfs sink将不会再次尝试关闭文件，</span><br><span class="line">                                             这个未关闭的文件将会一直留在那，并且是打开状态；</span><br><span class="line">                                             设置为0，当一次关闭失败后，hdfs sink会继续尝试下一次关闭，直到成功</span><br><span class="line">hdfs.retryInterval        180                hdfs sink 尝试关闭文件的时间间隔，</span><br><span class="line">                                             如果设置为0，表示不尝试，相当于于将hdfs.closeTries设置成1</span><br><span class="line">serializer               TEXT                序列化类型</span><br><span class="line">serializer.*                 </span><br></pre></td></tr></table></figure>
<p><strong>Avro Sink</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Property Name              Default              Description</span><br><span class="line">channel@                         –   </span><br><span class="line">type@                        –                  指定类型：avro.</span><br><span class="line">hostname@                    –                  主机名或IP</span><br><span class="line">port@                        –                  端口号</span><br><span class="line">batch-size                  100                 批次处理Event数</span><br><span class="line">connect-timeout            20000                连接超时时间</span><br><span class="line">request-timeout            20000                请求超时时间</span><br><span class="line">compression-type            none                压缩类型，“none” or “deflate”.</span><br><span class="line">compression-level            6                  压缩级别，0表示不压缩，1-9数字越大，压缩比越高</span><br><span class="line">ssl                        false                使用ssl加密</span><br></pre></td></tr></table></figure>
<p><strong>Kafka Sink</strong></p>
<ul>
<li>传输数据到Kafka中，需要注意的是Flume版本和Kafka版本的兼容性</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Property Name              Default             Description</span><br><span class="line">type                         –                 指定类型：org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">kafka.bootstrap.servers      –                 kafka服务地址</span><br><span class="line">kafka.topic          default-flume-topic       kafka Topic</span><br><span class="line">flumeBatchSize              100                批次写入kafka Event数</span><br><span class="line">kafka.producer.acks          1                 多少个副本确认后才能确定消息传递成功，0表示不需要确认</span><br><span class="line">                                               1表示只需要首要的副本得到确认，-1表示等待所有确认。</span><br></pre></td></tr></table></figure>
<h2 id="运行Flume"><a href="#运行Flume" class="headerlink" title="运行Flume"></a>运行Flume</h2><p>执行如下命令来开始Flume任务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf flume-conf 配置路径 --conf-file 采集任务配置文件 -name agent名 -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure>
<p>如下图所示，说明Flume开始了采集任务，</p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225931033-1937068728.png">  </p>
<p>如下图，是通过jar包产生的一组测试数据，当flume开始工作的时候，会不断输出如下所示数据流信息，同时将采集到数据存储到hdfs路径下，后面的后拽则是相应在HDFS中的文件名。</p>
<p><img src="https://img2020.cnblogs.com/blog/1620989/202101/1620989-20210122225940383-582032821.png"></p>
<p>   对于flume任务来说，在设定时间限制的情况下，不需要专门的去停止flume任务，否则利用ctrl+z强制停止，然后再使用kill 命令去kill掉进程，<code>ps -ef | grep XXX.conf</code> 利用该命令去查询相应的进程 然后kill即可。</p>
<p>​    主要是第一次使用Flume工具，因此，有很多的不懂，整个过程虽然简单，但是还是磕磕绊绊吧。下面篇我们利用Java来产生大量日志数据（继而调用相应的jar包  并通过flume监听 采集到HDFS中）。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Liruijia
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://liruijia.github.io/2021/01/17/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/" title="Flume日志采集">http://liruijia.github.io/2021/01/17/Flume日志采集/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/flume/" rel="tag"># flume</a>
              <a href="/tags/HDFS/" rel="tag"># HDFS</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/01/17/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" rel="prev" title="数据仓库">
                  <i class="fa fa-chevron-left"></i> 数据仓库
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/01/17/Flink-DataStream%20API%20%E5%AD%A6%E4%B9%A0/" rel="next" title="Flink-DataStream API 学习">
                  Flink-DataStream API 学习 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liruijia</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">36k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">33 分钟</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//unpkg.com/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//unpkg.com/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  
<script src="/js/local-search.js"></script>






  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



    <div class="pjax">


    </div>
</body>
</html>
